{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOGDDv1t8rGVh1gifxUgJcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shriyatha/Named_Entity_Recognition/blob/main/BERT_TELUGU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loZfieUlAboD"
      },
      "outputs": [],
      "source": [
        "# BERT-based Named Entity Recognition (NER) on CoNLL-2003 Dataset\n",
        "!pip install transformers datasets evaluate seqeval torch tqdm matplotlib pandas seaborn -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# @title 1. Data Loading and Exploration\n",
        "\n",
        "print(\"\\n==== Data Loading and Exploration ====\")\n",
        "\n",
        "# Load Telugu WikiANN dataset\n",
        "print(\"Loading Telugu WikiANN dataset...\")\n",
        "dataset = load_dataset(\"wikiann\", \"te\")\n",
        "metric = load(\"seqeval\")\n",
        "\n",
        "# Basic dataset info\n",
        "print(\"\\nDataset splits:\")\n",
        "for split in dataset.keys():\n",
        "    print(f\"- {split}: {dataset[split].num_rows} examples\")\n",
        "\n",
        "# Examine data structure\n",
        "print(\"\\nDataset features:\", dataset[\"train\"].features)\n",
        "\n",
        "# Get NER tag names and their meanings\n",
        "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "label_meanings = {\n",
        "    \"O\": \"Outside of a named entity\",\n",
        "    \"B-PER\": \"Beginning of person name\",\n",
        "    \"I-PER\": \"Inside of person name\",\n",
        "    \"B-ORG\": \"Beginning of organization name\",\n",
        "    \"I-ORG\": \"Inside of organization name\",\n",
        "    \"B-LOC\": \"Beginning of location name\",\n",
        "    \"I-LOC\": \"Inside of location name\"\n",
        "}\n",
        "\n",
        "print(\"\\nNER Tags:\")\n",
        "for i, label in enumerate(label_list):\n",
        "    meaning = label_meanings.get(label, \"\")\n",
        "    print(f\"{i}: {label:<6} - {meaning}\")\n",
        "\n",
        "# Check class distribution\n",
        "tag_counts = defaultdict(int)\n",
        "for example in dataset[\"train\"]:\n",
        "    for tag in example[\"ner_tags\"]:\n",
        "        tag_counts[label_list[tag]] += 1\n",
        "\n",
        "print(\"\\nTag distribution in training set:\")\n",
        "for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"- {tag:<6}: {count:,} tokens\")\n",
        "\n",
        "# @title Visualize Tag Distribution\n",
        "\n",
        "# Plot tag distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "tags = list(tag_counts.keys())\n",
        "counts = list(tag_counts.values())\n",
        "sns.barplot(x=tags, y=counts)\n",
        "plt.title(\"NER Tag Distribution in Training Set\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Tag\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display an example\n",
        "print(\"\\nExample from training set:\")\n",
        "example_idx = 42\n",
        "example = dataset[\"train\"][example_idx]\n",
        "for token, tag_idx in zip(example[\"tokens\"][:20], example[\"ner_tags\"][:20]):\n",
        "    print(f\"{token:<30} -> {label_list[tag_idx]}\")\n",
        "print(\"...\")\n",
        "\n",
        "# @title 2. Data Preprocessing\n",
        "\n",
        "print(\"\\n==== Data Preprocessing ====\")\n",
        "\n",
        "# Initialize tokenizer (for Telugu, we'll use multilingual BERT)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Preprocess function to align labels with tokens\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)  # Special tokens get -100\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])  # First token of word\n",
        "            else:\n",
        "                label_ids.append(-100)  # Subsequent tokens of word get -100\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n",
        "tokenization_time = time.time() - start_time\n",
        "print(f\"Tokenization completed in {tokenization_time:.2f} seconds\")\n",
        "\n",
        "# @title 3. Model Setup\n",
        "\n",
        "print(\"\\n==== Model Setup ====\")\n",
        "\n",
        "# Create data collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 16\n",
        "print(f\"Using batch size: {batch_size}\")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"test\"],\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Initialize multilingual BERT model for token classification\n",
        "print(\"\\nInitializing multilingual BERT model for token classification...\")\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=len(label_list),\n",
        "    id2label={i: label for i, label in enumerate(label_list)},\n",
        "    label2id={label: i for i, label in enumerate(label_list)}\n",
        ").to(device)\n",
        "\n",
        "# Print model architecture\n",
        "print(\"\\nModel architecture:\")\n",
        "print(model.__class__.__name__)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")\n",
        "\n",
        "# Helper function to extract entities\n",
        "def extract_entities(tokens, tags):\n",
        "    \"\"\"Extract entities from tokens and tags\"\"\"\n",
        "    entities = defaultdict(list)\n",
        "    current_entity = []\n",
        "    current_type = None\n",
        "\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        if tag.startswith('B-'):\n",
        "            # End previous entity if there was one\n",
        "            if current_entity:\n",
        "                entities[current_type].append(current_entity)\n",
        "                current_entity = []\n",
        "            # Start a new entity\n",
        "            current_type = tag[2:]  # Remove the B- prefix\n",
        "            current_entity = [token]\n",
        "        elif tag.startswith('I-') and current_entity and tag[2:] == current_type:\n",
        "            # Continue the current entity\n",
        "            current_entity.append(token)\n",
        "        elif tag == 'O':\n",
        "            # End previous entity if there was one\n",
        "            if current_entity:\n",
        "                entities[current_type].append(current_entity)\n",
        "                current_entity = []\n",
        "            current_type = None\n",
        "\n",
        "    # Add the last entity if there is one\n",
        "    if current_entity:\n",
        "        entities[current_type].append(current_entity)\n",
        "\n",
        "    return entities\n",
        "\n",
        "# @title 4. Training Setup\n",
        "\n",
        "print(\"\\n==== Training Setup ====\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 2e-5\n",
        "epochs = 5\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 0\n",
        "\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Epochs: {epochs}\")\n",
        "print(f\"Weight decay: {weight_decay}\")\n",
        "print(f\"Warmup steps: {warmup_steps}\")\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Helper function for evaluation\n",
        "def evaluate(dataloader, desc=\"Evaluating\"):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=desc):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            batch_preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            batch_labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "        for preds, labels in zip(batch_preds, batch_labels):\n",
        "            # Filter out ignored index (-100)\n",
        "            true_indices = [i for i, l in enumerate(labels) if l != -100]\n",
        "            true_labels.append([label_list[labels[i]] for i in true_indices])\n",
        "            predictions.append([label_list[preds[i]] for i in true_indices])\n",
        "\n",
        "    results = metric.compute(predictions=predictions, references=true_labels)\n",
        "    return results, predictions, true_labels\n",
        "\n",
        "# @title 5. Training Loop\n",
        "\n",
        "print(\"\\n==== Training Loop ====\")\n",
        "\n",
        "# Create output directory for results\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# Store metrics for plotting\n",
        "train_losses = []\n",
        "val_f1_scores = []\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "start_training_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    val_results, _, _ = evaluate(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\")\n",
        "    val_f1_scores.append(val_results[\"overall_f1\"])\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs} completed in {epoch_time:.2f} seconds\")\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Validation F1: {val_results['overall_f1']:.4f}\")\n",
        "    print(f\"Validation Precision: {val_results['overall_precision']:.4f}\")\n",
        "    print(f\"Validation Recall: {val_results['overall_recall']:.4f}\")\n",
        "\n",
        "total_training_time = time.time() - start_training_time\n",
        "print(f\"\\nTraining completed in {total_training_time:.2f} seconds ({total_training_time/60:.2f} minutes)\")\n",
        "\n",
        "# Save the model\n",
        "model_save_path = \"results/telugu_ner_model\"\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "# @title Visualize Training Progress\n",
        "\n",
        "# Plot training progress\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, epochs + 1), train_losses, marker='o')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, epochs + 1), val_f1_scores, marker='o', color='green')\n",
        "plt.title(\"Validation F1 Score\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# @title 6. Test Evaluation and Analysis\n",
        "\n",
        "print(\"\\n==== Test Evaluation and Analysis ====\")\n",
        "\n",
        "print(\"Evaluating on test set...\")\n",
        "test_results, test_predictions, test_true_labels = evaluate(test_dataloader, \"Testing\")\n",
        "\n",
        "# Print overall metrics\n",
        "print(\"\\nTest Results:\")\n",
        "print(f\"Accuracy: {test_results['overall_accuracy']:.4f}\")\n",
        "print(f\"Precision: {test_results['overall_precision']:.4f}\")\n",
        "print(f\"Recall: {test_results['overall_recall']:.4f}\")\n",
        "print(f\"F1 Score: {test_results['overall_f1']:.4f}\")\n",
        "\n",
        "# Print per-entity metrics\n",
        "print(\"\\nPer-Entity Type Metrics:\")\n",
        "entity_results = {}\n",
        "\n",
        "for key in sorted(test_results.keys()):\n",
        "    if key not in ['overall_accuracy', 'overall_precision', 'overall_recall', 'overall_f1']:\n",
        "        entity_results[key] = {\n",
        "            'precision': test_results[key]['precision'],\n",
        "            'recall': test_results[key]['recall'],\n",
        "            'f1': test_results[key]['f1'],\n",
        "            'number': test_results[key]['number']\n",
        "        }\n",
        "        print(f\"{key}:\")\n",
        "        print(f\" Precision: {test_results[key]['precision']:.4f}\")\n",
        "        print(f\" Recall: {test_results[key]['recall']:.4f}\")\n",
        "        print(f\" F1: {test_results[key]['f1']:.4f}\")\n",
        "        print(f\" Support: {test_results[key]['number']} entities\")\n",
        "\n",
        "# @title Visualize Entity-Level Performance\n",
        "\n",
        "# Plot per-entity metrics\n",
        "entity_df = pd.DataFrame.from_dict(entity_results, orient='index')\n",
        "entity_metrics = pd.DataFrame({\n",
        "    'Entity': entity_df.index,\n",
        "    'Precision': entity_df.precision,\n",
        "    'Recall': entity_df.recall,\n",
        "    'F1': entity_df.f1,\n",
        "    'Support': entity_df.number\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "ax = plt.subplot(111)\n",
        "\n",
        "# Plot bars\n",
        "bar_width = 0.25\n",
        "indices = np.arange(len(entity_metrics))\n",
        "ax.bar(indices - bar_width, entity_metrics['Precision'], bar_width,\n",
        "       label='Precision', color='blue', alpha=0.7)\n",
        "ax.bar(indices, entity_metrics['Recall'], bar_width,\n",
        "       label='Recall', color='green', alpha=0.7)\n",
        "ax.bar(indices + bar_width, entity_metrics['F1'], bar_width,\n",
        "       label='F1', color='red', alpha=0.7)\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_xlabel('Entity Type')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Performance by Entity Type')\n",
        "ax.set_xticks(indices)\n",
        "ax.set_xticklabels(entity_metrics['Entity'])\n",
        "ax.legend()\n",
        "\n",
        "# Add support count as text on top of bars\n",
        "for i, support in enumerate(entity_metrics['Support']):\n",
        "    ax.text(i, 0.95, f\"n={support}\", ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# @title 7. Error Analysis\n",
        "\n",
        "print(\"\\n==== Error Analysis ====\")\n",
        "\n",
        "model.eval()\n",
        "misclassified_examples = []\n",
        "test_set_indices = []\n",
        "batch_idx = 0\n",
        "\n",
        "for batch in tqdm(test_dataloader, desc=\"Collecting misclassified examples\"):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        batch_predictions = torch.argmax(logits, dim=-1)\n",
        "        batch_labels = batch[\"labels\"]\n",
        "\n",
        "    # Process each example in the batch\n",
        "    for i in range(len(batch_predictions)):\n",
        "        # Get the input IDs\n",
        "        input_ids = batch[\"input_ids\"][i].cpu().numpy()\n",
        "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "        token_preds = batch_predictions[i].cpu().numpy()\n",
        "        token_labels = batch_labels[i].cpu().numpy()\n",
        "\n",
        "        # Collect misclassified tokens\n",
        "        sample_misclassified = []\n",
        "        sample_index = batch_idx * batch_size + i\n",
        "\n",
        "        for j, (token, pred, label) in enumerate(zip(tokens, token_preds, token_labels)):\n",
        "            if label != -100:  # Skip special tokens and subtokens\n",
        "                pred_label = label_list[pred]\n",
        "                true_label = label_list[label]\n",
        "                if pred_label != true_label:\n",
        "                    sample_misclassified.append({\n",
        "                        'token': token,\n",
        "                        'pred': pred_label,\n",
        "                        'true': true_label\n",
        "                    })\n",
        "\n",
        "        if sample_misclassified:\n",
        "            misclassified_examples.append(sample_misclassified)\n",
        "            test_set_indices.append(sample_index)\n",
        "\n",
        "    batch_idx += 1\n",
        "\n",
        "print(f\"\\nFound {len(misclassified_examples)} examples with misclassifications\")\n",
        "\n",
        "# Analyze error types\n",
        "error_types = defaultdict(int)\n",
        "error_tokens = defaultdict(list)\n",
        "\n",
        "for example in misclassified_examples:\n",
        "    for item in example:\n",
        "        error_type = (item['true'], item['pred'])\n",
        "        error_types[error_type] += 1\n",
        "        error_tokens[error_type].append(item['token'])\n",
        "\n",
        "# Print most common error types\n",
        "print(\"\\nMost common error types:\")\n",
        "for (true, pred), count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "    tokens_sample = error_tokens[(true, pred)][:3]  # Show a few example tokens\n",
        "    print(f\"TRUE: {true:<6} -> PRED: {pred:<6}: {count} occurrences\")\n",
        "    print(f\" Example tokens: {', '.join(tokens_sample)}\")\n",
        "\n",
        "# @title Visualize Confusion Matrix\n",
        "\n",
        "# Create confusion matrix for the most common classes\n",
        "most_common_labels = [label for label in label_list if not label.startswith('I-')] + ['I-PER', 'I-ORG']\n",
        "label_to_idx = {label: idx for idx, label in enumerate(most_common_labels)}\n",
        "confusion_matrix = np.zeros((len(most_common_labels), len(most_common_labels)))\n",
        "\n",
        "for (true, pred), count in error_types.items():\n",
        "    if true in most_common_labels and pred in most_common_labels:\n",
        "        confusion_matrix[label_to_idx[true], label_to_idx[pred]] += count\n",
        "\n",
        "# Add correct predictions to diagonal\n",
        "for pred_batch, true_batch in zip(test_predictions, test_true_labels):\n",
        "    for pred, true in zip(pred_batch, true_batch):\n",
        "        if pred == true and pred in most_common_labels:\n",
        "            confusion_matrix[label_to_idx[pred], label_to_idx[pred]] += 1\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='.0f',\n",
        "            xticklabels=most_common_labels,\n",
        "            yticklabels=most_common_labels)\n",
        "plt.title('Confusion Matrix (Most Common Classes)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print examples of specific error patterns\n",
        "# Example: \"O\" predicted as an entity\n",
        "print(\"\\nExamples of tokens predicted as entities but are not:\")\n",
        "o_as_entity = [(true, pred) for (true, pred) in error_types.keys()\n",
        "              if true == 'O' and pred != 'O']\n",
        "for error_type in o_as_entity[:3]:\n",
        "    true, pred = error_type\n",
        "    tokens = error_tokens[error_type][:5]\n",
        "    print(f\"TRUE: {true} -> PRED: {pred}:\")\n",
        "    print(f\" Tokens: {', '.join(tokens)}\")\n",
        "\n",
        "# Example: Entity predicted as \"O\"\n",
        "print(\"\\nExamples of entities missed (predicted as 'O'):\")\n",
        "entity_as_o = [(true, pred) for (true, pred) in error_types.keys()\n",
        "              if true != 'O' and pred == 'O']\n",
        "for error_type in entity_as_o[:3]:\n",
        "    true, pred = error_type\n",
        "    tokens = error_tokens[error_type][:5]\n",
        "    print(f\"TRUE: {true} -> PRED: {pred}:\")\n",
        "    print(f\" Tokens: {', '.join(tokens)}\")\n",
        "\n",
        "# Example: Entity type confusion\n",
        "print(\"\\nExamples of entity type confusion:\")\n",
        "entity_confusion = [(true, pred) for (true, pred) in error_types.keys()\n",
        "                   if true != 'O' and pred != 'O' and true != pred]\n",
        "for error_type in entity_confusion[:3]:\n",
        "    true, pred = error_type\n",
        "    tokens = error_tokens[error_type][:5]\n",
        "    print(f\"TRUE: {true} -> PRED: {pred}:\")\n",
        "    print(f\" Tokens: {', '.join(tokens)}\")\n",
        "\n",
        "# @title 8. Model Performance Analysis\n",
        "\n",
        "print(\"\\n==== Model Performance Analysis ====\")\n",
        "\n",
        "# Analyze performance based on token length\n",
        "print(\"Analyzing performance by token length:\")\n",
        "\n",
        "# Collect performance by token length\n",
        "length_performance = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
        "\n",
        "for i, (preds, trues) in enumerate(zip(test_predictions, test_true_labels)):\n",
        "    # Convert string lengths to bins\n",
        "    length_bin = min(len(preds) // 5 * 5 + 5, 50)  # bin by 5 tokens, cap at 50+\n",
        "    correct = sum(p == t for p, t in zip(preds, trues))\n",
        "    length_performance[length_bin]['correct'] += correct\n",
        "    length_performance[length_bin]['total'] += len(preds)\n",
        "\n",
        "# Calculate accuracy by length\n",
        "length_accuracy = {length: stats['correct'] / stats['total']\n",
        "                  for length, stats in length_performance.items()\n",
        "                  if stats['total'] > 0}\n",
        "\n",
        "# @title Visualize Accuracy by Sentence Length\n",
        "\n",
        "# Plot accuracy by sentence length\n",
        "plt.figure(figsize=(10, 6))\n",
        "lengths = sorted(length_accuracy.keys())\n",
        "accuracies = [length_accuracy[l] for l in lengths]\n",
        "plt.bar(range(len(lengths)), accuracies, align='center')\n",
        "plt.xticks(range(len(lengths)), [f\"â‰¤{l}\" for l in lengths])\n",
        "plt.xlabel(\"Sentence Length (tokens)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracy by Sentence Length\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze entity boundary detection\n",
        "print(\"\\nAnalyzing entity boundary detection:\")\n",
        "\n",
        "boundary_errors = {\n",
        "    'B->I': 0,  # Beginning predicted as Inside\n",
        "    'I->B': 0,  # Inside predicted as Beginning\n",
        "    'entity->O': 0,  # Entity boundaries missed completely\n",
        "    'O->entity': 0  # Non-entities predicted as entities\n",
        "}\n",
        "\n",
        "for pred_batch, true_batch in zip(test_predictions, test_true_labels):\n",
        "    for pred, true in zip(pred_batch, true_batch):\n",
        "        # Analyze boundary errors\n",
        "        if true.startswith('B-') and pred.startswith('I-'):\n",
        "            boundary_errors['B->I'] += 1\n",
        "        elif true.startswith('I-') and pred.startswith('B-'):\n",
        "            boundary_errors['I->B'] += 1\n",
        "        elif (true.startswith('B-') or true.startswith('I-')) and pred == 'O':\n",
        "            boundary_errors['entity->O'] += 1\n",
        "        elif true == 'O' and (pred.startswith('B-') or pred.startswith('I-')):\n",
        "            boundary_errors['O->entity'] += 1\n",
        "\n",
        "# Print boundary errors\n",
        "print(\"Entity boundary errors:\")\n",
        "for error_type, count in boundary_errors.items():\n",
        "    print(f\"- {error_type}: {count} occurrences\")\n",
        "\n",
        "# @title Visualize Boundary Errors\n",
        "\n",
        "# Plot boundary errors\n",
        "plt.figure(figsize=(10, 6))\n",
        "error_types = list(boundary_errors.keys())\n",
        "error_counts = list(boundary_errors.values())\n",
        "plt.bar(error_types, error_counts, color='coral')\n",
        "plt.xlabel(\"Error Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Entity Boundary Detection Errors\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# @title Generate Summary Report\n",
        "\n",
        "# Generate a summary report\n",
        "with open(\"results/summary_report.txt\", \"w\") as f:\n",
        "    f.write(\"BERT-based NER Model for Telugu WikiANN Dataset\\n\")\n",
        "    f.write(\"==============================================\\n\\n\")\n",
        "    f.write(f\"Training completed in {total_training_time/60:.2f} minutes\\n\\n\")\n",
        "    f.write(\"Test Results:\\n\")\n",
        "    f.write(f\"Accuracy: {test_results['overall_accuracy']:.4f}\\n\")\n",
        "    f.write(f\"Precision: {test_results['overall_precision']:.4f}\\n\")\n",
        "    f.write(f\"Recall: {test_results['overall_recall']:.4f}\\n\")\n",
        "    f.write(f\"F1 Score: {test_results['overall_f1']:.4f}\\n\\n\")\n",
        "    f.write(\"Entity-wise Performance:\\n\")\n",
        "    for key in sorted(entity_results.keys()):\n",
        "        f.write(f\"{key}:\\n\")\n",
        "        f.write(f\"  Precision: {entity_results[key]['precision']:.4f}\\n\")\n",
        "        f.write(f\"  Recall: {entity_results[key]['recall']:.4f}\\n\")\n",
        "        f.write(f\"  F1: {entity_results[key]['f1']:.4f}\\n\")\n",
        "        f.write(f\"  Support: {entity_results[key]['number']} entities\\n\\n\")\n",
        "\n",
        "print(f\"\\nResults saved to the 'results' directory\")\n",
        "print(\"Summary report generated at: results/summary_report.txt\")\n"
      ],
      "metadata": {
        "id": "zP7qsa9AAsWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}